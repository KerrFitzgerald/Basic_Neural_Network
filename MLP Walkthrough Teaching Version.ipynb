{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-classics",
   "metadata": {},
   "source": [
    "# MLP Walkthough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-roommate",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-dragon",
   "metadata": {},
   "source": [
    "# Function to read .csv file data into a Pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_read(datafile):\n",
    "    df = pd.read_csv(datafile, delimiter = ',')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-multiple",
   "metadata": {},
   "source": [
    "# Function to Z score normalise the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(df):\n",
    "    df_std = df.copy()\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-bahrain",
   "metadata": {},
   "source": [
    "# Define softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-tradition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    s = np.exp(Z)\n",
    "    softmax = s/np.sum(s, axis=1, keepdims=True)\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-machinery",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss(Y,Y_hat):\n",
    "    minval = 0.000000000001             # Value to avoid zero division errors\n",
    "    m = Y.shape[0]\n",
    "    loss = (-1.0/m) * np.sum(Y * np.log(Y_hat.clip(min=minval)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-passage",
   "metadata": {},
   "source": [
    "# Define softmax loss derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_derivative(Y,Y_hat):\n",
    "    loss_deriv = (Y_hat-Y)\n",
    "    return loss_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-hydrogen",
   "metadata": {},
   "source": [
    "# Define tanh derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_derivative(x):\n",
    "    tanh_deriv = (1.0 - np.power(np.tanh(x), 2))\n",
    "    return tanh_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-schema",
   "metadata": {},
   "source": [
    "# Function to create network layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, nn_h1, nn_h2, Y):\n",
    "    nn_x = X.shape[1] # size of input layer\n",
    "    nn_y = Y.shape[1] # size of output layer\n",
    "    return nn_x, nn_h1, nn_h2, nn_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-reconstruction",
   "metadata": {},
   "source": [
    "# Function to initialize W & b values for each network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_parameters(nn_X, nn_h1, nn_h2, nn_Y):\n",
    "    \n",
    "    # Initialize weight & bias matrices for Layer 1\n",
    "    W1 = np.random.randn(nn_X, nn_h1) \n",
    "    b1 = np.zeros((1, nn_h1)) \n",
    "    # Initialize weight & bias matrices for Layer 2\n",
    "    W2 = np.random.randn(nn_h1, nn_h2)\n",
    "    b2 = np.zeros((1, nn_h2))\n",
    "    # Initialize weight & bias matrices for Layer 3 (Ouput Layer)\n",
    "    W3 = np.random.randn(nn_h2, nn_Y)\n",
    "    b3 = np.zeros((1, nn_Y))\n",
    "    \n",
    "    parameters = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3': W3,'b3': b3}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-cruise",
   "metadata": {},
   "source": [
    "# Function to apply forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "\n",
    "    # Retrieve each parameter from the \"parameters\" dictionary\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # 1st layer linear step\n",
    "    Z1 = (X).dot(W1) + b1\n",
    "    # 1st layer tanh activation function\n",
    "    A1 = np.tanh(Z1)\n",
    "    \n",
    "    # 2nd layer linear step\n",
    "    Z2 = A1.dot(W2) + b2\n",
    "    # 2nd layer tanh activation function\n",
    "    A2 = np.tanh(Z2)\n",
    "    \n",
    "    # 3rd layer linear step\n",
    "    Z3 = A2.dot(W3) + b3\n",
    "    # 3rd layer softmax activation function\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    cache = {'Z1': Z1,'A1': A1,'Z2': Z2,'A2': A2,'Z3': Z3,'A3': A3}\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-large",
   "metadata": {},
   "source": [
    "# Function to apply backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(X, parameters, cache, Y):\n",
    "    \n",
    "    # Retrieve each parameter from the \"parameters\" dictionary\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # Retrieve Z & A values from the \"cache\" dictionary\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z3 = cache[\"Z3\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "   \n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    # 3rd layer derivatives:\n",
    "    dZ3 = softmax_loss_derivative(Y,A3)\n",
    "    dW3 = (1.0/m)*(A2.T).dot(dZ3)\n",
    "    db3 = (1.0/m)*np.sum(dZ3, axis=0)\n",
    "    \n",
    "    # 2nd layer derivatives:\n",
    "    dZ2 = np.multiply(dZ3.dot(W3.T), tanh_derivative(Z2))\n",
    "    dW2 = (1.0/m)*np.dot(A1.T, dZ2)\n",
    "    db2 = (1.0/m)*np.sum(dZ2, axis=0)\n",
    "    \n",
    "    # 1st layer derivatives:\n",
    "    dZ1 = np.multiply(dZ2.dot(W2.T),tanh_derivative(Z1))\n",
    "    dW1 = (1.0/m)*np.dot(X.T,dZ1)\n",
    "    db1 = (1.0/m)*np.sum(dZ1,axis=0)\n",
    "    \n",
    "    # Store gradients in dictionary\n",
    "    grads = {'dW3': dW3, 'db3': db3, 'dW2': dW2,'db2': db2,'dW1': dW1,'db1': db1}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-sucking",
   "metadata": {},
   "source": [
    "# Function to update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    # Retrieve each parameter from the \"parameters\" dictionary\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "\n",
    "    # Retrieve each derivative from the \"grads\" dictionary\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "\n",
    "    # Update 3rd layer parameters using derivatives\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    \n",
    "    # Update 2nd layer parameters using derivatives\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    # Update 1st layer parameters using derivatives\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2,\"b2\": b2, \"W3\": W3, \"b3\": b3}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-republican",
   "metadata": {},
   "source": [
    "# Bring everything together to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parameters, X, Y, learning_rate, epochs):\n",
    "    epoch_array  =  np.zeros(epochs)\n",
    "    losses_array =  np.zeros(epochs)\n",
    "    accur_array  =  np.zeros(epochs)\n",
    "    for i in range(0, epochs):\n",
    "        # Apply forward propagtion\n",
    "        cache = forward_prop(X, parameters)                           \n",
    "        # Apply backpopagation\n",
    "        grads = backward_prop(X, parameters, cache, Y)\n",
    "        # Update parameters using derivatives\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # Calculate the cost/loss function\n",
    "        A3 = cache['A3']\n",
    "        Y_hat = np.argmax(A3, axis=1)\n",
    "        Y_true = Y.argmax(axis=1)\n",
    "        loss = softmax_loss(Y,A3)\n",
    "        epoch_array[i] = i\n",
    "        losses_array[i] = loss\n",
    "        accur_array[i] = accuracy_score(Y_hat,Y_true)*100\n",
    "        if i % 500 == 0:\n",
    "            print('Loss after epoch        ',i,':',loss)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(Y_hat,Y_true)*100,'%')\n",
    "    \n",
    "    return parameters, epoch_array, losses_array, accur_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-light",
   "metadata": {},
   "source": [
    "# Read in data, seperate class labels & encode class labels to create Y\n",
    "# CAREFUL THIS MUST BE IN THE CORRECT FORMAT!\n",
    "# The classification column must be called 'target' & range from 0 to N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('wine.data')\n",
    "df = pd.read_csv('iris_data.csv')\n",
    "\n",
    "Y_classes = df.pop('target')\n",
    "Y_classes = Y_classes.to_numpy()\n",
    "num_classes = np.max(Y_classes+1)\n",
    "\n",
    "print('Number of classes:', num_classes)\n",
    "print('Length of Y_classes array:', len(Y_classes))\n",
    "Y = np.zeros((len(Y_classes), num_classes))\n",
    "print('Length of Y array:', len(Y))\n",
    "print('Shape of Y array:', Y.shape)\n",
    "for i in range(0,len(Y)):\n",
    "    for j in range(0,num_classes):\n",
    "        if Y_classes[i] == j:\n",
    "            Y[i][j] = 1\n",
    "#print(Y)\n",
    "X = z_score(df).to_numpy()\n",
    "print('Length of X array:', (len(X)))\n",
    "print('Shape of X array:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-teddy",
   "metadata": {},
   "source": [
    "# Split into Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-boost",
   "metadata": {},
   "source": [
    "# Set up network for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "nn_x, h1, h2, nn_y = layer_sizes(X_train, 3, 4, Y_train)\n",
    "parameters = network_parameters(nn_x, h1, h2, nn_y)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Input features:', nn_x)\n",
    "print('Neurons in hidden layer 1:', h1)\n",
    "print('Neurons in hidden layer 2:', h2)\n",
    "print('Output features:', nn_y)\n",
    "print('W1 shape:', parameters['W1'].shape )\n",
    "#print(parameters['W1'])\n",
    "print('b1 shape:', parameters['b1'].shape )\n",
    "#print(parameters['b1'])\n",
    "print('W2 shape:', parameters['W2'].shape )\n",
    "#print(parameters['W2'])\n",
    "print('b2 shape:', parameters['b2'].shape )\n",
    "#print(parameters['b2'])\n",
    "print('W3 shape:', parameters['W3'].shape )\n",
    "#print(parameters['W3'])\n",
    "print('b3 shape:', parameters['b3'].shape )\n",
    "#print(parameters['b3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-patch",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, epochs, losses, accuracy = train(parameters, X_train, Y_train, learning_rate=0.005,epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-satisfaction",
   "metadata": {},
   "source": [
    "# Plot the loss vs the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-minnesota",
   "metadata": {},
   "source": [
    "# Plot the accuracy vs the epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-sydney",
   "metadata": {},
   "source": [
    "# Evaluate the performance of the neural network on the test set\n",
    "# (Backpropagation isn't used! We are using the final network parameters after training has finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = forward_prop(X_test, model)\n",
    "A3 = model_test['A3']\n",
    "print(A3.shape)\n",
    "Y_hat = np.argmax(A3, axis=1)\n",
    "Y_true = Y_test.argmax(axis=1)\n",
    "Test_accuracy = accuracy_score(Y_hat,Y_true)*100\n",
    "print('Test_accuracy', Test_accuracy, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
